{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanz\\AppData\\Local\\Temp\\ipykernel_29016\\4107407487.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['field_goal_result'].fillna('none', inplace=True)\n",
      "C:\\Users\\seanz\\AppData\\Local\\Temp\\ipykernel_29016\\4107407487.py:521: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '548.7808836884615' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dfV5.at[index, 'Home_Momentum_Score'] = dfV5.at[index - 1, 'Home_Momentum_Score'] + home_momentum_gain\n",
      "C:\\Users\\seanz\\AppData\\Local\\Temp\\ipykernel_29016\\4107407487.py:522: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '489.3937295340633' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dfV5.at[index, 'Away_Momentum_Score'] = dfV5.at[index - 1, 'Away_Momentum_Score'] + away_momentum_gain\n",
      "C:\\Users\\seanz\\AppData\\Local\\Temp\\ipykernel_29016\\4107407487.py:575: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dfV5 = dfV5.groupby('game_id', group_keys=False).apply(detect_momentum_shifts)\n",
      "C:\\Users\\seanz\\AppData\\Local\\Temp\\ipykernel_29016\\4107407487.py:583: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dfV5[columns_to_fill] = dfV5[columns_to_fill].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('NFL_pbp_2009-2019.csv', low_memory=False)\n",
    "\n",
    "threshold = 100000\n",
    "df['field_goal_result'].fillna('none', inplace=True)\n",
    "dfV2 = df.loc[:, df.isnull().sum() < threshold]\n",
    "missing_values = dfV2.isnull().sum()\n",
    "\n",
    "statistical_cols = ['play_id', 'game_id', 'home_team', 'away_team', 'posteam', \n",
    "                    'defteam', 'side_of_field', 'yardline_100', 'half_seconds_remaining', \n",
    "                    'game_seconds_remaining', 'game_half', 'drive', 'qtr', 'down', 'goal_to_go', 'time', \n",
    "                    'yrdln', 'ydstogo', 'ydsnet', 'desc', 'play_type', 'yards_gained', 'home_timeouts_remaining', \n",
    "                    'away_timeouts_remaining', 'total_home_score',  'total_away_score', 'score_differential', 'home_wp', 'away_wp', 'ep']\n",
    "\n",
    "game_dynamics_cols = [\n",
    "    'punt_blocked', 'first_down_rush', 'first_down_pass', 'first_down_penalty', 'third_down_converted',\n",
    "    'third_down_failed', 'fourth_down_converted', 'fourth_down_failed', 'incomplete_pass', 'interception',\n",
    "    'fumble_forced', 'fumble_not_forced', 'fumble_out_of_bounds', 'solo_tackle', 'safety', 'penalty',\n",
    "    'tackled_for_loss', 'fumble_lost', 'own_kickoff_recovery', 'own_kickoff_recovery_td', 'qb_hit',\n",
    "    'rush_attempt', 'pass_attempt', 'sack', 'touchdown', 'pass_touchdown', 'rush_touchdown', 'field_goal_result',\n",
    "    'return_touchdown', 'extra_point_attempt', 'two_point_attempt', 'field_goal_attempt', 'kickoff_attempt',\n",
    "    'punt_attempt', 'fumble', 'complete_pass', 'shotgun', 'no_huddle', 'punt_inside_twenty', 'kickoff_inside_twenty']\n",
    "\n",
    "columns_to_keep = statistical_cols + game_dynamics_cols\n",
    "dfV3 = dfV2[columns_to_keep]\n",
    "\n",
    "dfV4 = dfV3.drop(['play_id', 'game_seconds_remaining', 'fumble_forced'], axis=1)\n",
    "dfV4 = dfV4.dropna(subset=['down', 'defteam', 'posteam'])\n",
    "dfV4 = dfV4.reset_index(drop=True)\n",
    "\n",
    "# Indicators for if within last 2 minutes of the half and the whole game\n",
    "dfV4['close_to_end_of_half'] = (dfV4['half_seconds_remaining'] <= 120).astype(int)\n",
    "dfV4['close_to_end_of_game'] = ((dfV4['half_seconds_remaining'] <= 120) & (dfV4['game_half'] == 'Half2')).astype(int)\n",
    "\n",
    "# Indicator for if the touchdown was for the away or home team\n",
    "dfV4['home_td'] = ((dfV4['touchdown'] == 1) & (dfV4['posteam'] != dfV4['away_team'])).astype(int)\n",
    "dfV4['away_td'] = ((dfV4['touchdown'] == 1) & (dfV4['posteam'] != dfV4['home_team'])).astype(int)\n",
    "\n",
    "# Trackers for the difference in both teams' win probability after each play\n",
    "dfV4['home_wp_change'] = dfV4['home_wp'].diff().fillna(0)\n",
    "dfV4['away_wp_change'] = dfV4['away_wp'].diff().fillna(0)\n",
    "\n",
    "# Indicator for turnover\n",
    "dfV4['turnover'] = (\n",
    "    (dfV4['safety'] == 1) |\n",
    "    (dfV4['interception'] == 1) |\n",
    "    (dfV4['fumble_lost'] == 1) |\n",
    "    ((dfV4['fourth_down_converted'] == 0) & (dfV4['down'] == 4))\n",
    ").astype(int)\n",
    "\n",
    "# Drive time - Added drive ended indicator to help - Manually resets after end of game, half, and change of possession\n",
    "dfV4['drive_ended'] = (\n",
    "    (dfV4['posteam'] != dfV4['posteam'].shift(1)) |  \n",
    "    (dfV4['game_id'] != dfV4['game_id'].shift(1)) |  \n",
    "    dfV4['desc'].str.contains('END GAME', na=False) |  \n",
    "    dfV4['desc'].str.contains('END QUARTER', na=False)  \n",
    ").astype(int)\n",
    "dfV4['drive'] = (\n",
    "    (dfV4['posteam'].ne(dfV4['posteam'].shift())) |\n",
    "    (dfV4['game_id'].ne(dfV4['game_id'].shift()))\n",
    ").cumsum()\n",
    "dfV4['drive_time_seconds'] = (\n",
    "    dfV4.groupby(['game_id', 'drive'])['half_seconds_remaining']\n",
    "    .transform('first') - dfV4['half_seconds_remaining']\n",
    ")\n",
    "dfV4['drive_time_seconds'] = dfV4.apply(\n",
    "    lambda row: 0 if row['drive_ended'] == 1 else row['drive_time_seconds'], axis=1\n",
    ")\n",
    "dfV4['drive_time_seconds'] = dfV4.groupby(['game_id', 'drive'])['drive_time_seconds'].cumsum()\n",
    "\n",
    "# Indicator for long touchdowns\n",
    "dfV4['long_td'] = ((dfV4['touchdown'] == 1) & (dfV4['yards_gained'] >= 50)).astype(int)\n",
    "\n",
    "# Trackers for score differentials and lead changes\n",
    "dfV4['home_score_differential'] = dfV4['total_home_score'] - dfV4['total_away_score']\n",
    "dfV4['away_score_differential'] = -dfV4['home_score_differential']\n",
    "dfV4['lead_change'] = ((dfV4['home_score_differential'].diff() < 0) &\n",
    "                       (dfV4['home_score_differential'].shift() * dfV4['home_score_differential'] < 0)).astype(int)\n",
    "\n",
    "# Combining first down indicators\n",
    "dfV4['first_down'] = ((dfV4['first_down_pass'] == 1) | (dfV4['first_down_rush'] == 1) | (dfV4['first_down_penalty'] == 1)).astype(int)\n",
    "\n",
    "# Indicators for scoring drives - Removing\n",
    "dfV4['home_scoring_drive'] = (\n",
    "    (dfV4['home_td'] == 1) \n",
    ").astype(int)\n",
    "dfV4['away_scoring_drive'] = (\n",
    "    (dfV4['away_td'] == 1) \n",
    ").astype(int)\n",
    "\n",
    "# Helper for consecutive scoring events - Remove Later!!!!!!!!!!!!!!\n",
    "dfV4['home_scoring_events'] = (\n",
    "    (dfV4['posteam'] != dfV4['away_team']) & \n",
    "    ((dfV4['home_td'] == 1) | (dfV4['field_goal_result'] == 'made'))\n",
    ").astype(int)\n",
    "dfV4['away_scoring_events'] = (\n",
    "    (dfV4['posteam'] != dfV4['home_team']) & \n",
    "    ((dfV4['away_td'] == 1) | (dfV4['field_goal_result'] == 'made'))\n",
    ").astype(int)\n",
    "\n",
    "# Consecutive Scoring Events + Helper function \n",
    "def calc_consecutive_cumsum_with_game_reset(series, reset_series, game_ids):\n",
    "    cumsum = 0\n",
    "    consecutive = []\n",
    "    prev_game_id = None  \n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if game_ids[i] != prev_game_id:\n",
    "            cumsum = 0 \n",
    "        if reset_series[i] == 1:  \n",
    "            cumsum = 0\n",
    "        if series[i] == 1:  \n",
    "            cumsum += 1\n",
    "        consecutive.append(cumsum)\n",
    "        prev_game_id = game_ids[i]  \n",
    "    return consecutive\n",
    "\n",
    "dfV4['home_csum_scores'] = calc_consecutive_cumsum_with_game_reset(\n",
    "    dfV4['home_scoring_events'], dfV4['away_scoring_events'], dfV4['game_id']\n",
    ")\n",
    "dfV4['away_csum_scores'] = calc_consecutive_cumsum_with_game_reset(\n",
    "    dfV4['away_scoring_events'], dfV4['home_scoring_events'], dfV4['game_id']\n",
    ")\n",
    "\n",
    "#Consecutive defensive stops\n",
    "dfV4['home_def_stop'] = (\n",
    "    (dfV4['posteam'] != dfV4['home_team']) &  ((dfV4['punt_attempt'] == 1) |  (dfV4['turnover'] == 1)) & \n",
    "    ~dfV4['field_goal_result'].isin(['made'])  \n",
    ").astype(int)\n",
    "dfV4['away_def_stop'] = (\n",
    "    (dfV4['posteam'] != dfV4['away_team']) & ((dfV4['punt_attempt'] == 1) |  (dfV4['turnover'] == 1)) & \n",
    "    ~dfV4['field_goal_result'].isin(['made'])\n",
    ").astype(int)\n",
    "\n",
    "def calc_consecutive_defensive_stops_with_game_reset(series, reset_series, game_ids):\n",
    "    cumsum = 0\n",
    "    consecutive = []\n",
    "    prev_game_id = None  \n",
    "    for i in range(len(series)):\n",
    "        if game_ids[i] != prev_game_id:\n",
    "            cumsum = 0\n",
    "        if reset_series[i] == 1:\n",
    "            cumsum = 0\n",
    "        if series[i] == 1:\n",
    "            cumsum += 1\n",
    "        consecutive.append(cumsum)\n",
    "        prev_game_id = game_ids[i]  \n",
    "    return consecutive\n",
    "\n",
    "dfV4['home_csum_def_stops'] = calc_consecutive_defensive_stops_with_game_reset(\n",
    "    dfV4['home_def_stop'], dfV4['away_scoring_events'], dfV4['game_id']\n",
    ")\n",
    "dfV4['away_csum_def_stops'] = calc_consecutive_defensive_stops_with_game_reset(\n",
    "    dfV4['away_def_stop'], dfV4['home_scoring_events'], dfV4['game_id']\n",
    ")\n",
    "\n",
    "# Home/Away Drive Numbers\n",
    "dfV4['away_drive_number'] = (\n",
    "    dfV4.loc[dfV4['posteam'] != dfV4['home_team']]\n",
    "    .groupby('game_id')['drive_ended'].cumsum()\n",
    ")\n",
    "dfV4['home_drive_number'] = (\n",
    "    dfV4.loc[dfV4['posteam'] == dfV4['home_team']]\n",
    "    .groupby('game_id')['drive_ended'].cumsum()\n",
    ")\n",
    "\n",
    "# Offense needs to score\n",
    "dfV4['off_need_score'] = (\n",
    "    (dfV4['down'].isin([3, 4])) & \n",
    "    (abs(dfV4['score_differential']) <= 8) & \n",
    "    (dfV4['qtr'] >= 4) &\n",
    "    (dfV4['first_down'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Defense Needs a Stop\n",
    "dfV4['def_need_stop'] = (\n",
    "    (dfV4['down'].isin([3, 4])) & \n",
    "    (abs(dfV4['score_differential']) <= 8) & \n",
    "    (dfV4['qtr'] >= 4) &\n",
    "    (dfV4['turnover'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Drought Ending score\n",
    "dfV4['drought_end_play'] = (\n",
    "    ((dfV4['away_csum_scores'].shift(1) >= 2) & (dfV4['away_csum_scores'] == 0) & (dfV4['home_scoring_events'] == 1)) |\n",
    "    ((dfV4['home_csum_scores'].shift(1) >= 2) & (dfV4['home_csum_scores'] == 0) & (dfV4['away_scoring_events'] == 1))\n",
    ").astype(int)\n",
    "\n",
    "# Defensive touchdown\n",
    "dfV4['def_td'] = (\n",
    "    ((dfV4['fumble'] == 1) & (dfV4['return_touchdown'] == 1)) |\n",
    "    ((dfV4['interception'] == 1) & (dfV4['return_touchdown'] == 1))\n",
    ").astype(int)\n",
    "\n",
    "# Defensive touchdown\n",
    "dfV4['off_td'] = (\n",
    "    (dfV4['pass_touchdown'] == 1) | (dfV4['rush_touchdown'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Special Teams touchdown\n",
    "dfV4['st_return_td'] = (\n",
    "    ((dfV4['kickoff_attempt'] == 1) & (dfV4['return_touchdown'] == 1)) | \n",
    "    ((dfV4['punt_attempt'] == 1) & (dfV4['return_touchdown'] == 1))  \n",
    ").astype(int)\n",
    "\n",
    "# Big special teams play...punt blocked, field goal blocked, return_touchdown, kick recovery, pin team near endzone\n",
    "dfV4['big_st_play'] = (\n",
    "    (dfV4['punt_blocked'] == 1) | \n",
    "    (dfV4['field_goal_result'] == 'blocked') | \n",
    "    (dfV4['own_kickoff_recovery'] == 1) | \n",
    "    (dfV4['st_return_td'] == 1) | \n",
    "    (dfV4['kickoff_inside_twenty'] == 1) | \n",
    "    (dfV4['punt_inside_twenty'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Scoring type differentiatior, touchdowns should hold more weight than a field goal, other types may hold more weight also\n",
    "dfV4['scoring_type'] = np.select(\n",
    "    [\n",
    "        dfV4['field_goal_result'] == 'made',\n",
    "        dfV4['off_td'] == 1,\n",
    "        dfV4['def_td'] == 1,\n",
    "        dfV4['st_return_td'] == 1,\n",
    "    ],\n",
    "    ['fg', 'off_td', 'def_td', 'st_td'],\n",
    "    default='none'\n",
    ")\n",
    "\n",
    "# Indicator for big offensive play\n",
    "dfV4['big_offensive_play'] = (\n",
    "        (dfV4['yards_gained'] >= 40) |\n",
    "        (dfV4['long_td'] == 1) |\n",
    "        ((dfV4['off_need_score'] == 1) & (dfV4['off_td'] == 1))\n",
    ").astype(int)\n",
    "\n",
    "# Indicator for big defensive play\n",
    "dfV4['big_defensive_play'] = (\n",
    "    (dfV4['sack'] == 1) |\n",
    "    (dfV4['tackled_for_loss'] == 1) |\n",
    "    ((dfV4['def_need_stop'] == 1) & ((dfV4['def_td'] == 'def_td')) | dfV4['turnover'] == 1) |\n",
    "    (dfV4['scoring_type'] == 'def_td')\n",
    ").astype(int)\n",
    "\n",
    "#Quick Score and Quick Stop #### Needs fixing, only want 1 on last play of drive when they score or get stop, right now 1 for whole drive\n",
    "dfV4['total_drive_time'] = dfV4.groupby('drive')['drive_time_seconds'].transform('last') \n",
    "dfV4['cumulative_drive_time'] = dfV4.groupby(['game_id', 'drive'])['drive_time_seconds'].cumsum()\n",
    "dfV4['long_drive_triggered'] = (\n",
    "    dfV4.groupby(['game_id', 'drive'])['cumulative_drive_time']\n",
    "    .transform(lambda x: (x > 360).idxmax() == x.index)  # Flags the first row that exceeds 360s\n",
    ").astype(int)\n",
    "\n",
    "dfV4['quick_score'] = (\n",
    "    (dfV4['drive_time_seconds'] < 180) &\n",
    "    ((dfV4['touchdown'] == 1) | (dfV4['field_goal_result'] == 'made')) &\n",
    "    (dfV4.groupby('drive')['drive_time_seconds'].transform('last') == dfV4['drive_time_seconds'])\n",
    ").astype(int)\n",
    "dfV4['quick_stop'] = (\n",
    "    (dfV4['total_drive_time'] < 180) & \n",
    "    (dfV4['scoring_type'] == 'none') &\n",
    "    (dfV4.groupby('drive')['drive_time_seconds'].transform('last') == dfV4['drive_time_seconds'])\n",
    ").astype(int)\n",
    "\n",
    "# Consecutive first downs\n",
    "dfV4['home_csum_first_downs'] = 0\n",
    "dfV4['away_csum_first_downs'] = 0\n",
    "dfV4['home_csum_first_downs'] = (\n",
    "    dfV4.groupby(['home_team', 'away_team', 'home_drive_number'])['first_down']\n",
    "    .cumsum()\n",
    "    .where(dfV4['posteam'] != 'away_team', 0)\n",
    ")\n",
    "dfV4['away_csum_first_downs'] = (\n",
    "    dfV4.groupby(['home_team', 'away_team', 'away_drive_number'])['first_down']\n",
    "    .cumsum()\n",
    "    .where(dfV4['posteam'] != 'home_team', 0)\n",
    ")\n",
    "\n",
    "\n",
    "columns_to_remove = [\n",
    "    'ep', 'punt_blocked', 'first_down_rush', 'first_down_pass', \n",
    "    'third_down_converted', 'third_down_failed', 'fourth_down_converted', \n",
    "    'fourth_down_failed', 'incomplete_pass', 'interception', 'fumble_not_forced', \n",
    "    'fumble_out_of_bounds', 'solo_tackle', 'safety', 'penalty', 'tackled_for_loss', \n",
    "    'fumble_lost', 'own_kickoff_recovery', 'own_kickoff_recovery_td', 'qb_hit', \n",
    "    'rush_attempt', 'pass_attempt', 'sack', 'extra_point_attempt', 'two_point_attempt', \n",
    "    'field_goal_attempt', 'kickoff_attempt', 'punt_attempt', 'fumble', 'pass_touchdown', 'rush_touchdown'\n",
    "    'complete_pass', 'shotgun', 'home_scoring_drive', 'away_scoring_drive','home_scoring_events','away_scoring_events',\n",
    "    'rush_touchdown', 'field_goal_result', 'return_touchdown', 'complete_pass', 'no_huddle', 'punt_inside_twenty', 'kickoff_inside_twenty',\n",
    "    'time', 'yrdln', 'ydstogo', 'ydsnet', 'desc', 'side_of_field', 'yardline_100', 'desc', 'drive', 'game_half', 'drive_ended', 'drive_time_seconds',\n",
    "    'touchdown', 'score_differential', 'total_drive_time'\n",
    "]\n",
    "\n",
    "dfV5 = dfV4.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "dynamics = [\n",
    "    ('big_offensive_play', dfV5['big_offensive_play'] == 1),\n",
    "    ('big_defensive_play', dfV5['big_defensive_play'] == 1),\n",
    "    ('off_td', dfV5['off_td'] == 1),\n",
    "    ('def_td', dfV5['def_td'] == 1),\n",
    "    ('big_st_play', dfV5['big_st_play'] == 1),\n",
    "    ('st_return_td', dfV5['st_return_td'] == 1),\n",
    "    ('off_need_score', dfV5['off_need_score'] == 1),\n",
    "    ('def_need_stop', dfV5['def_need_stop'] == 1),\n",
    "    ('drought_end_play', dfV5['drought_end_play'] == 1),\n",
    "    ('home_csum_scores', dfV5['home_csum_scores'] >= 2),\n",
    "    ('away_csum_scores', dfV5['away_csum_scores'] >= 2),\n",
    "    ('home_csum_def_stops', dfV5['home_csum_def_stops'] >= 2),\n",
    "    ('away_csum_def_stops', dfV5['away_csum_def_stops'] >= 2),\n",
    "    ('home_csum_first_downs', dfV5['home_csum_first_downs'] >= 2),\n",
    "    ('away_csum_first_downs', dfV5['away_csum_first_downs'] >= 2),\n",
    "    ('long_td', dfV5['long_td'] == 1),\n",
    "    ('quick_score', dfV5['quick_score'] == 1),\n",
    "    ('quick_stop', dfV5['quick_stop'] == 1),\n",
    "    ('home_score_differential', dfV5['home_score_differential'] == 1),\n",
    "    ('away_score_differential', dfV5['away_score_differential'] == 1),\n",
    "]\n",
    "\n",
    "\n",
    "def_wp_change = {\n",
    "    \"big_defensive_play\": 0.029471,\n",
    "    \"def_td\": 0.016322,\n",
    "    \"big_st_play\": 0.034637,\n",
    "    \"st_return_td\": 0.040082,\n",
    "    \"def_need_stop\": 0.042132,\n",
    "    \"quick_stop\": 0.029971\n",
    "}\n",
    "\n",
    "off_wp_change = {\n",
    "    \"big_offensive_play\": 0.038602,\n",
    "    \"off_td\": 0.028432,\n",
    "    \"off_need_score\":  0.035536, \n",
    "    \"drought_end_play\": 0.028891,\n",
    "    \"long_td\": 0.033325,\n",
    "    \"quick_score\": 0.026664\n",
    "}\n",
    "\n",
    "streaks_multipliers = {\n",
    "    \"home_csum_scores\": 1.118986,\n",
    "    \"away_csum_scores\": 1.118986,\n",
    "    \"home_csum_first_downs\": 1.1112094,\n",
    "    \"away_csum_first_downs\": 1.1112094,\n",
    "    \"home_csum_def_stops\": 1.111932,\n",
    "    \"away_csum_def_stops\": 1.111932,\n",
    "}\n",
    "\n",
    "score_game_multipliers = {\n",
    "    \"tied_or_1_score\": 1.06634844,\n",
    "    \"2_score\": 1.035777727,\n",
    "    \"3_or_more_score\": 1.0274060\n",
    "}\n",
    "\n",
    "qtr_multipliers = {\n",
    "    \"first_and_fourth\": 1.5522285,\n",
    "    \"second_and_third\": 1.3201836\n",
    "}\n",
    "\n",
    "home_away_multipliers = {\n",
    "    \"home\": 1.07949869,  \n",
    "    \"away\": 1.06027507 \n",
    "}\n",
    "\n",
    "boost_case_multipliers = {\n",
    "    \"home_and_4th\": 1.122276683,  \n",
    "    \"away_and_1st\": 1.16675933,  \n",
    "    \"none\": 1.0           \n",
    "}\n",
    "\n",
    "decay_multipliers = {\n",
    "    \"opponent_scores\": 0.68004571,\n",
    "    \"turnover\": 0.21742678,\n",
    "    \"opponent_ends_drought\": 0.18212307,\n",
    "    \"long_possession\":  0.1534018395,\n",
    "    \"none\": 0.0  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def calculate_multipliers(row, index, category, is_offensive):\n",
    "    if abs(row['home_score_differential']) <= 8:\n",
    "        S = score_game_multipliers[\"tied_or_1_score\"]\n",
    "    elif 9 <= abs(row['home_score_differential']) <= 16:\n",
    "        S = score_game_multipliers[\"2_score\"]\n",
    "    else:\n",
    "        S = score_game_multipliers[\"3_or_more_score\"]\n",
    "\n",
    "    team = row['posteam'] if is_offensive else row['defteam']\n",
    "    HA = home_away_multipliers.get(team, 1.0)\n",
    "\n",
    "    if row['qtr'] == 1 or row['qtr'] == 4:\n",
    "        Q = qtr_multipliers[\"first_and_fourth\"]\n",
    "    else:\n",
    "        Q = qtr_multipliers[\"second_and_third\"]\n",
    "\n",
    "    if is_offensive:\n",
    "        if row['posteam'] == 'home' and row['qtr'] == 4:\n",
    "            B = boost_case_multipliers[\"home_and_4th\"]\n",
    "        elif row['posteam'] == 'away' and row['qtr'] == 1:\n",
    "            B = boost_case_multipliers[\"away_and_1st\"]\n",
    "        else:\n",
    "            B = 1.0\n",
    "    else:\n",
    "        if team == 'home' and row['qtr'] == 4:\n",
    "            B = boost_case_multipliers[\"home_and_4th\"]\n",
    "        elif team == 'away' and row['qtr'] == 1:\n",
    "            B = boost_case_multipliers[\"away_and_1st\"]\n",
    "        else:\n",
    "            B = 1.0\n",
    "\n",
    "    CS = 1.0\n",
    "   \n",
    "    if is_offensive:\n",
    "        if row['posteam'] == row['home_team']:\n",
    "            if row['home_csum_scores'] >= 2:\n",
    "                if row['home_csum_scores'] > dfV5.at[index - 1, 'home_csum_scores']: \n",
    "                    CS = streaks_multipliers['home_csum_scores']\n",
    "            elif row['home_csum_first_downs'] >= 4:\n",
    "                if row['home_csum_first_downs'] > dfV5.at[index - 1, 'home_csum_first_downs']: \n",
    "                    CS = streaks_multipliers['home_csum_first_downs']\n",
    "            else:\n",
    "                CS = 1.0\n",
    "        else:\n",
    "            if row['away_csum_scores'] >= 2:\n",
    "                if row['away_csum_scores'] > dfV5.at[index - 1, 'away_csum_scores']: \n",
    "                    CS = streaks_multipliers['away_csum_scores']\n",
    "            elif row['away_csum_first_downs'] >= 4:\n",
    "                if row['away_csum_first_downs'] >  dfV5.at[index - 1, 'away_csum_first_downs']: \n",
    "                    CS = streaks_multipliers['away_csum_first_downs']\n",
    "            else:\n",
    "                CS = 1.0\n",
    "    else:\n",
    "        if row['defteam'] == row['home_team']:\n",
    "            if row['home_csum_def_stops'] >= 2:\n",
    "                if row['home_csum_def_stops'] > dfV5.at[index - 1, 'home_csum_def_stops']: \n",
    "                    CS = streaks_multipliers['home_csum_def_stops']\n",
    "            else:\n",
    "                CS = 1.0\n",
    "        else:\n",
    "            if row['away_csum_def_stops'] >= 2:\n",
    "                if row['away_csum_def_stops'] > dfV5.at[index - 1, 'away_csum_def_stops']:\n",
    "                    CS = streaks_multipliers['away_csum_def_stops']\n",
    "            else:\n",
    "                CS = 1.0\n",
    "\n",
    "    return S, HA, B, CS, Q\n",
    "\n",
    "\n",
    "\n",
    "def calculate_momentum_gain(wp_change_value, S, HA, CS, B, Q):\n",
    "    return wp_change_value * (S * HA * CS * B * Q) * 1000\n",
    "\n",
    "\n",
    "\n",
    "def calculate_decay(row, category, momentum_gain):\n",
    "    if category in ['off_td', 'long_td', 'def_td', 'st_return_td']:\n",
    "        D = decay_multipliers['opponent_scores']\n",
    "    elif row['turnover'] == 1:\n",
    "        D = decay_multipliers['turnover']\n",
    "    elif row['drought_end_play'] == 1:\n",
    "        D = decay_multipliers[\"opponent_ends_drought\"]\n",
    "    elif row['long_drive_triggered'] == 1:  \n",
    "        D = decay_multipliers['long_possession']\n",
    "    else:\n",
    "        D = decay_multipliers['none']\n",
    "\n",
    "    return momentum_gain * D\n",
    "\n",
    "\n",
    "\n",
    "def update_momentum_scores(dfV5):\n",
    "    dfV5['Home_Momentum_Score'] = 500\n",
    "    dfV5['Away_Momentum_Score'] = 500\n",
    "\n",
    "    dfV5['game_id_diff'] = dfV5['game_id'] != dfV5['game_id'].shift(1)\n",
    "\n",
    "    for index, row in dfV5.iterrows():\n",
    "        if index == 0:  \n",
    "            continue\n",
    "\n",
    "        if row['game_id_diff']:\n",
    "            dfV5.at[index, 'Home_Momentum_Score'] = 500\n",
    "            dfV5.at[index, 'Away_Momentum_Score'] = 500\n",
    "            continue\n",
    "\n",
    "        home_momentum_gain = 0\n",
    "        away_momentum_gain = 0\n",
    "\n",
    "        for category, wp_change_value in off_wp_change.items():\n",
    "            if row[category] == 1:\n",
    "                S, HA, B, CS, Q = calculate_multipliers(row, index, category, True)\n",
    "                momentum_gain = calculate_momentum_gain(wp_change_value, S, HA, CS, B, Q)\n",
    "                momentum_loss = calculate_decay(row, category, momentum_gain)\n",
    "\n",
    "                if row['posteam'] == row['home_team']:\n",
    "                    home_momentum_gain += momentum_gain\n",
    "                    away_momentum_gain -= momentum_loss\n",
    "                else:\n",
    "                    away_momentum_gain += momentum_gain\n",
    "                    home_momentum_gain -= momentum_loss\n",
    "\n",
    "        for category, wp_change_value in def_wp_change.items():\n",
    "            if row[category] == 1:\n",
    "                S, HA, B, CS, Q = calculate_multipliers(row, index, category, False)\n",
    "                momentum_gain = calculate_momentum_gain(wp_change_value, S, HA, CS, B, Q)\n",
    "                momentum_loss = calculate_decay(row, category, momentum_gain)\n",
    "\n",
    "                if row['defteam'] == row['home_team']:\n",
    "                    home_momentum_gain += momentum_gain\n",
    "                    away_momentum_gain -= momentum_loss\n",
    "                else:\n",
    "                    away_momentum_gain += momentum_gain\n",
    "                    home_momentum_gain -= momentum_loss\n",
    "\n",
    "        dfV5.at[index, 'Home_Momentum_Score'] = dfV5.at[index - 1, 'Home_Momentum_Score'] + home_momentum_gain\n",
    "        dfV5.at[index, 'Away_Momentum_Score'] = dfV5.at[index - 1, 'Away_Momentum_Score'] + away_momentum_gain\n",
    "\n",
    "update_momentum_scores(dfV5)\n",
    "\n",
    "dfV5['Game_Momentum_Diff'] = 0\n",
    "\n",
    "historical_max_diff_mean = dfV5.groupby('game_id')['Game_Momentum_Diff'].max().mean()\n",
    "historical_max_diff_std = dfV5.groupby('game_id')['Game_Momentum_Diff'].max().std()\n",
    "\n",
    "base_threshold = historical_max_diff_mean + 0.8 * historical_max_diff_std #.7\n",
    "\n",
    "dfV5['Game_Momentum_Diff'] = abs(dfV5['Home_Momentum_Score'] - dfV5['Away_Momentum_Score'])\n",
    "dfV5['Dynamic_Threshold'] = None\n",
    "dfV5['Momentum_Holding_Team'] = None\n",
    "\n",
    "def detect_momentum_shifts(game_data):\n",
    "    momentum_holding_team = None\n",
    "    last_shift_home_momentum = game_data.iloc[0]['Home_Momentum_Score']\n",
    "    last_shift_away_momentum = game_data.iloc[0]['Away_Momentum_Score']\n",
    "    max_momentum_diff_so_far = 0\n",
    "\n",
    "    for i in range(1, len(game_data)): \n",
    "        if i < 10:  # Ignore shifts for the first 10 plays\n",
    "            continue\n",
    "        home_momentum_diff = game_data.iloc[i]['Home_Momentum_Score'] - last_shift_home_momentum\n",
    "        away_momentum_diff = game_data.iloc[i]['Away_Momentum_Score'] - last_shift_away_momentum        \n",
    "\n",
    "        current_momentum_diff = abs(game_data.iloc[i]['Home_Momentum_Score'] - game_data.iloc[i]['Away_Momentum_Score'])\n",
    "        max_momentum_diff_so_far = max(max_momentum_diff_so_far, current_momentum_diff)\n",
    "        game_threshold = max(base_threshold, 0.8 * max_momentum_diff_so_far) #.7\n",
    "        game_data.iloc[i, game_data.columns.get_loc('Dynamic_Threshold')] = game_threshold\n",
    "\n",
    "        home_momentum_shift = False\n",
    "        away_momentum_shift = False\n",
    "\n",
    "        if home_momentum_diff >= game_threshold and away_momentum_diff < game_threshold * 0.8: #.5\n",
    "            home_momentum_shift = True\n",
    "        elif away_momentum_diff >= game_threshold and home_momentum_diff < game_threshold * 0.8: #.5\n",
    "            away_momentum_shift = True\n",
    "\n",
    "        if home_momentum_shift:\n",
    "            momentum_holding_team = \"Home\"\n",
    "            last_shift_home_momentum = game_data.iloc[i]['Home_Momentum_Score']\n",
    "            last_shift_away_momentum = game_data.iloc[i]['Away_Momentum_Score']\n",
    "        elif away_momentum_shift:\n",
    "            momentum_holding_team = \"Away\"\n",
    "            last_shift_home_momentum = game_data.iloc[i]['Home_Momentum_Score']\n",
    "            last_shift_away_momentum = game_data.iloc[i]['Away_Momentum_Score']\n",
    "\n",
    "        game_data.iloc[i, game_data.columns.get_loc('Momentum_Holding_Team')] = momentum_holding_team\n",
    "\n",
    "    return game_data\n",
    "\n",
    "dfV5 = dfV5.groupby('game_id', group_keys=False).apply(detect_momentum_shifts)\n",
    "\n",
    "dfV5['Momentum_Shift_Occurred'] = dfV5.groupby('game_id')['Momentum_Holding_Team'].transform(\n",
    "    lambda x: x.ne(x.shift()) & x.notna()\n",
    ")\n",
    "\n",
    "columns_to_fill = ['home_drive_number', 'away_drive_number', 'home_csum_first_downs', \n",
    "                    'away_csum_first_downs', 'Dynamic_Threshold', 'yards_gained']\n",
    "dfV5[columns_to_fill] = dfV5[columns_to_fill].fillna(0)\n",
    "\n",
    "features = dfV5.drop(['Momentum_Shift_Occurred'], axis=1)  \n",
    "numeric_df = dfV5.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "df = dfV5.copy()\n",
    "\n",
    "excluded_cols = ['game_id', 'Momentum_Shift_Occurred', 'Momentum_Holding_Team']\n",
    "target_col = 'Momentum_Shift_Occurred'\n",
    "feature_df = df.drop(columns=excluded_cols, errors='ignore')\n",
    "\n",
    "numeric_cols = feature_df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = feature_df.select_dtypes(exclude=['number']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    feature_df = pd.get_dummies(feature_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "unique_games = df['game_id'].unique()\n",
    "unique_games_sorted = sorted(unique_games)\n",
    "train_size = int(0.8 * len(unique_games_sorted))\n",
    "train_games = unique_games_sorted[:train_size]\n",
    "test_games = unique_games_sorted[train_size:]\n",
    "\n",
    "feature_df['game_id'] = df['game_id']\n",
    "train_features = feature_df[feature_df['game_id'].isin(train_games)].drop(columns='game_id')\n",
    "test_features = feature_df[feature_df['game_id'].isin(test_games)].drop(columns='game_id')\n",
    "X_train = train_features.values\n",
    "X_test = test_features.values\n",
    "y_train = df.loc[df['game_id'].isin(train_games), target_col].astype(int).values\n",
    "y_test = df.loc[df['game_id'].isin(test_games), target_col].astype(int).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Results So Far:\n",
    "## MLP - Hypertuned Parameters: hidden_layer_sizes=(32, 16), alpha=0.001, solver='adam', max_iter=500, random_state=42, Pred_Threshold tuned to: 0.16842105263157897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set with Corrected ±4 Play Window ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    331300\n",
      "           1       0.69      0.77      0.73      6975\n",
      "\n",
      "    accuracy                           0.99    338275\n",
      "   macro avg       0.84      0.88      0.86    338275\n",
      "weighted avg       0.99      0.99      0.99    338275\n",
      "\n",
      "\n",
      "=== Test Set with Corrected ±4 Play Window ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     82061\n",
      "           1       0.51      0.42      0.46      1726\n",
      "\n",
      "    accuracy                           0.98     83787\n",
      "   macro avg       0.75      0.71      0.73     83787\n",
      "weighted avg       0.98      0.98      0.98     83787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import maximum_filter1d\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(32, 16), alpha=0.001, solver='adam', max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_proba = mlp.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "y_train_pred_threshold = (y_train_proba >= 0.16842105263157897).astype(int)\n",
    "y_test_pred_threshold = (y_test_proba >= 0.16842105263157897).astype(int)\n",
    "\n",
    "def apply_temporal_correction(preds, actuals, window_size=4):\n",
    "    corrected_preds = np.zeros_like(preds)\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(preds), i + window_size + 1)\n",
    "        if preds[i] == 1 and np.any(actuals[start:end] == 1):\n",
    "            corrected_preds[i] = 1 \n",
    "    return corrected_preds\n",
    "\n",
    "y_train_pred_corrected = apply_temporal_correction(y_train_pred_threshold, y_train)\n",
    "y_test_pred_corrected = apply_temporal_correction(y_test_pred_threshold, y_test)\n",
    "\n",
    "print(\"\\n=== Train Set with Corrected ±4 Play Window ===\")\n",
    "print(classification_report(y_train, y_train_pred_corrected, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set with Corrected ±4 Play Window ===\")\n",
    "print(classification_report(y_test, y_test_pred_corrected, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to try training after researching:\n",
    "#### XGBoost - worked well with big data class project, handles imblanced data well\n",
    "#### LSTM - views plays as sequence of events rather than individual events\n",
    "#### Temporal CNN (1D Convolutional Neural Network) - Also treats plays as sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# XGBoost Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Classification Threshold Found: 0.50 (F1-Score: 0.5066) ===\n",
      "\n",
      "=== Train Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    331300\n",
      "           1       0.78      1.00      0.88      6975\n",
      "\n",
      "    accuracy                           0.99    338275\n",
      "   macro avg       0.89      1.00      0.94    338275\n",
      "weighted avg       1.00      0.99      0.99    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     82061\n",
      "           1       0.49      0.52      0.51      1726\n",
      "\n",
      "    accuracy                           0.98     83787\n",
      "   macro avg       0.74      0.75      0.75     83787\n",
      "weighted avg       0.98      0.98      0.98     83787\n",
      "\n",
      "\n",
      "=== Train Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    331300\n",
      "           1       0.86      1.00      0.93      6975\n",
      "\n",
      "    accuracy                           1.00    338275\n",
      "   macro avg       0.93      1.00      0.96    338275\n",
      "weighted avg       1.00      1.00      1.00    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     82061\n",
      "           1       0.70      0.52      0.60      1726\n",
      "\n",
      "    accuracy                           0.99     83787\n",
      "   macro avg       0.84      0.76      0.79     83787\n",
      "weighted avg       0.98      0.99      0.98     83787\n",
      "\n",
      "\n",
      "=== Best XGBoost Hyperparameters ===\n",
      "{'subsample': 0.6, 'scale_pos_weight': 7, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 12, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Expanded search space for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],  # More trees to test deeper models\n",
    "    \"max_depth\": [3, 6, 9, 12],  # Allow deeper trees\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1],  # More refined learning rates\n",
    "    \"subsample\": [0.6, 0.7, 0.8, 0.9],  # Sample more aggressively\n",
    "    \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9],  # Feature sampling per tree\n",
    "    \"scale_pos_weight\": [3, 5, 7, 10],  # Stronger balancing for class imbalance\n",
    "    \"min_child_weight\": [1, 3, 5, 7],  # Controls minimum sum of weights in a child node\n",
    "    \"gamma\": [0, 0.1, 0.3, 0.5],  # Minimum loss reduction (pruning)\n",
    "}\n",
    "\n",
    "# Define a range of classification thresholds\n",
    "threshold_values = np.linspace(0.1, 0.9, 9)  # Test thresholds from 0.1 to 0.9\n",
    "\n",
    "# Initialize XGBoost with parallelization turned off\n",
    "xgb = XGBClassifier(n_jobs=1, random_state=42)\n",
    "\n",
    "# Use RandomizedSearchCV for efficient tuning\n",
    "xgb_grid_search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    scoring=\"f1\",  # Optimize for F1-score due to class imbalance\n",
    "    cv=3,  # More robust cross-validation\n",
    "    n_iter=15,  # Test 15 random combinations\n",
    "    n_jobs=1  # Disable parallel processing to avoid CPU overload\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from tuning\n",
    "best_xgb = xgb_grid_search.best_estimator_\n",
    "\n",
    "# Make probability predictions\n",
    "y_train_proba = best_xgb.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = best_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === Optimize the Classification Threshold ===\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    y_train_pred_threshold = (y_train_proba >= threshold).astype(int)\n",
    "    y_test_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    \n",
    "    f1 = classification_report(y_test, y_test_pred_threshold, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n=== Best Classification Threshold Found: {best_threshold:.2f} (F1-Score: {best_f1:.4f}) ===\")\n",
    "\n",
    "# Apply Best Threshold for Predictions\n",
    "y_train_pred_threshold = (y_train_proba >= best_threshold).astype(int)\n",
    "y_test_pred_threshold = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# === Apply Temporal Correction ===\n",
    "def apply_temporal_correction(preds, actuals, window_size=4):\n",
    "    corrected_preds = np.zeros_like(preds)\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(preds), i + window_size + 1)\n",
    "        if preds[i] == 1 and np.any(actuals[start:end] == 1):\n",
    "            corrected_preds[i] = 1  \n",
    "    return corrected_preds\n",
    "\n",
    "y_train_pred_corrected = apply_temporal_correction(y_train_pred_threshold, y_train)\n",
    "y_test_pred_corrected = apply_temporal_correction(y_test_pred_threshold, y_test)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\n=== Train Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Train Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_corrected, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_corrected, zero_division=0))\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\n=== Best XGBoost Hyperparameters ===\")\n",
    "print(xgb_grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with {'lstm_units': 64, 'dropout': 0.2, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 13ms/step - accuracy: 0.9770 - loss: 0.0920 - val_accuracy: 0.9809 - val_loss: 0.0726\n",
      "Epoch 2/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.9809 - loss: 0.0712 - val_accuracy: 0.9808 - val_loss: 0.0714\n",
      "Epoch 3/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0682 - val_accuracy: 0.9812 - val_loss: 0.0707\n",
      "Epoch 4/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 10ms/step - accuracy: 0.9813 - loss: 0.0651 - val_accuracy: 0.9812 - val_loss: 0.0710\n",
      "Epoch 5/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 10ms/step - accuracy: 0.9820 - loss: 0.0618 - val_accuracy: 0.9813 - val_loss: 0.0721\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step\n",
      "F1-score: 0.2360\n",
      "\n",
      "Training with {'lstm_units': 128, 'dropout': 0.3, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 17ms/step - accuracy: 0.9774 - loss: 0.0902 - val_accuracy: 0.9799 - val_loss: 0.0747\n",
      "Epoch 2/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 15ms/step - accuracy: 0.9806 - loss: 0.0721 - val_accuracy: 0.9807 - val_loss: 0.0723\n",
      "Epoch 3/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 15ms/step - accuracy: 0.9805 - loss: 0.0702 - val_accuracy: 0.9811 - val_loss: 0.0709\n",
      "Epoch 4/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 15ms/step - accuracy: 0.9808 - loss: 0.0673 - val_accuracy: 0.9810 - val_loss: 0.0716\n",
      "Epoch 5/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 16ms/step - accuracy: 0.9815 - loss: 0.0632 - val_accuracy: 0.9811 - val_loss: 0.0730\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step\n",
      "F1-score: 0.2473\n",
      "\n",
      "Training with {'lstm_units': 64, 'dropout': 0.3, 'lr': 0.005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 12ms/step - accuracy: 0.9784 - loss: 0.0905 - val_accuracy: 0.9807 - val_loss: 0.0764\n",
      "Epoch 2/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 11ms/step - accuracy: 0.9803 - loss: 0.0769 - val_accuracy: 0.9798 - val_loss: 0.0763\n",
      "Epoch 3/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 11ms/step - accuracy: 0.9798 - loss: 0.0766 - val_accuracy: 0.9806 - val_loss: 0.0756\n",
      "Epoch 4/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 11ms/step - accuracy: 0.9805 - loss: 0.0750 - val_accuracy: 0.9809 - val_loss: 0.0759\n",
      "Epoch 5/5\n",
      "\u001b[1m10571/10571\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 11ms/step - accuracy: 0.9800 - loss: 0.0762 - val_accuracy: 0.9807 - val_loss: 0.0756\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step\n",
      "F1-score: 0.2688\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step\n",
      "\n",
      "=== Best Hyperparameters ===\n",
      "{'lstm_units': 64, 'dropout': 0.3, 'lr': 0.005}\n",
      "\n",
      "=== Final Test Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     82051\n",
      "           1       0.60      0.17      0.27      1726\n",
      "\n",
      "    accuracy                           0.98     83777\n",
      "   macro avg       0.79      0.59      0.63     83777\n",
      "weighted avg       0.97      0.98      0.98     83777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "sequence_length = 10  \n",
    "threshold = 0.5 \n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])  \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, sequence_length)\n",
    "\n",
    "param_grid = [\n",
    "    {\"lstm_units\": 64, \"dropout\": 0.2, \"lr\": 0.001},\n",
    "    {\"lstm_units\": 128, \"dropout\": 0.3, \"lr\": 0.001},\n",
    "    {\"lstm_units\": 64, \"dropout\": 0.3, \"lr\": 0.005}\n",
    "]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for params in param_grid:\n",
    "    print(f\"\\nTraining with {params}\")\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(params[\"lstm_units\"], return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        LSTM(params[\"lstm_units\"]),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=params[\"lr\"]),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train_seq, y_train_seq,\n",
    "              validation_data=(X_test_seq, y_test_seq),\n",
    "              epochs=5, batch_size=32, verbose=1,\n",
    "              callbacks=[early_stop])\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_seq).flatten()\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    f1 = classification_report(y_test_seq, y_pred, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "y_final_proba = best_model.predict(X_test_seq).flatten()\n",
    "y_final_pred = (y_final_proba >= threshold).astype(int)\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(best_params)\n",
    "\n",
    "print(\"\\n=== Final Test Performance ===\")\n",
    "print(classification_report(y_test_seq, y_final_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training TCN with params: {'filt': 8, 'kernel_s': 5, 'dropout': 0.3, 'learning_rate': 0.005}\n",
      "WARNING:tensorflow:From c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:179: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 16ms/step - accuracy: 0.8120 - loss: 0.6363 - val_accuracy: 0.8891 - val_loss: 0.6190\n",
      "Epoch 2/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 17ms/step - accuracy: 0.8831 - loss: 0.5791 - val_accuracy: 0.8860 - val_loss: 0.4846\n",
      "Epoch 3/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 18ms/step - accuracy: 0.8829 - loss: 0.5723 - val_accuracy: 0.8855 - val_loss: 0.5121\n",
      "Epoch 4/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 17ms/step - accuracy: 0.8820 - loss: 0.5696 - val_accuracy: 0.8858 - val_loss: 0.5182\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step\n",
      "Best Threshold: 0.5, F1-Score: 0.1705\n",
      "\n",
      "Training TCN with params: {'filt': 8, 'kernel_s': 5, 'dropout': 0.3, 'learning_rate': 0.005}\n",
      "Epoch 1/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 16ms/step - accuracy: 0.8061 - loss: 0.6236 - val_accuracy: 0.8891 - val_loss: 0.6271\n",
      "Epoch 2/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 17ms/step - accuracy: 0.8845 - loss: 0.5757 - val_accuracy: 0.8891 - val_loss: 0.5339\n",
      "Epoch 3/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 16ms/step - accuracy: 0.8835 - loss: 0.5629 - val_accuracy: 0.8888 - val_loss: 0.5300\n",
      "Epoch 4/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 16ms/step - accuracy: 0.8835 - loss: 0.5812 - val_accuracy: 0.8862 - val_loss: 0.5176\n",
      "Epoch 5/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 17ms/step - accuracy: 0.8832 - loss: 0.5730 - val_accuracy: 0.8858 - val_loss: 0.5047\n",
      "Epoch 6/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 17ms/step - accuracy: 0.8834 - loss: 0.5781 - val_accuracy: 0.8862 - val_loss: 0.5063\n",
      "Epoch 7/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 17ms/step - accuracy: 0.8843 - loss: 0.5709 - val_accuracy: 0.8858 - val_loss: 0.5080\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step\n",
      "Best Threshold: 0.4, F1-Score: 0.1706\n",
      "\n",
      "Training TCN with params: {'filt': 16, 'kernel_s': 5, 'dropout': 0.2, 'learning_rate': 0.001}\n",
      "Epoch 1/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 24ms/step - accuracy: 0.7701 - loss: 0.6758 - val_accuracy: 0.8876 - val_loss: 0.5565\n",
      "Epoch 2/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 26ms/step - accuracy: 0.8800 - loss: 0.5804 - val_accuracy: 0.8858 - val_loss: 0.4778\n",
      "Epoch 3/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 26ms/step - accuracy: 0.8816 - loss: 0.5683 - val_accuracy: 0.8855 - val_loss: 0.5361\n",
      "Epoch 4/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 26ms/step - accuracy: 0.8830 - loss: 0.5735 - val_accuracy: 0.8855 - val_loss: 0.5489\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step\n",
      "Best Threshold: 0.5, F1-Score: 0.1706\n",
      "\n",
      "Training TCN with params: {'filt': 32, 'kernel_s': 5, 'dropout': 0.2, 'learning_rate': 0.001}\n",
      "Epoch 1/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 37ms/step - accuracy: 0.7733 - loss: 0.6677 - val_accuracy: 0.8858 - val_loss: 0.5588\n",
      "Epoch 2/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 39ms/step - accuracy: 0.8809 - loss: 0.5704 - val_accuracy: 0.8857 - val_loss: 0.5510\n",
      "Epoch 3/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 38ms/step - accuracy: 0.8821 - loss: 0.5660 - val_accuracy: 0.8855 - val_loss: 0.5493\n",
      "Epoch 4/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 38ms/step - accuracy: 0.8828 - loss: 0.5706 - val_accuracy: 0.8855 - val_loss: 0.5649\n",
      "Epoch 5/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 38ms/step - accuracy: 0.8823 - loss: 0.5624 - val_accuracy: 0.8855 - val_loss: 0.5598\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step\n",
      "Best Threshold: 0.5, F1-Score: 0.1703\n",
      "\n",
      "Training TCN with params: {'filt': 16, 'kernel_s': 5, 'dropout': 0.2, 'learning_rate': 0.005}\n",
      "Epoch 1/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 26ms/step - accuracy: 0.8282 - loss: 0.6166 - val_accuracy: 0.8859 - val_loss: 0.5441\n",
      "Epoch 2/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 26ms/step - accuracy: 0.8814 - loss: 0.5701 - val_accuracy: 0.8858 - val_loss: 0.5555\n",
      "Epoch 3/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 27ms/step - accuracy: 0.8806 - loss: 0.5686 - val_accuracy: 0.8856 - val_loss: 0.5175\n",
      "Epoch 4/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 27ms/step - accuracy: 0.8831 - loss: 0.5742 - val_accuracy: 0.8891 - val_loss: 0.5084\n",
      "Epoch 5/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 26ms/step - accuracy: 0.8820 - loss: 0.5798 - val_accuracy: 0.8855 - val_loss: 0.5282\n",
      "Epoch 6/15\n",
      "\u001b[1m10572/10572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 27ms/step - accuracy: 0.8832 - loss: 0.5657 - val_accuracy: 0.8857 - val_loss: 0.5702\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step\n",
      "Best Threshold: 0.4, F1-Score: 0.1708\n",
      "\n",
      "=== Best Hyperparameters & Threshold ===\n",
      "{'filt': 16, 'kernel_s': 5, 'dropout': 0.2, 'learning_rate': 0.005, 'threshold': 0.4}\n",
      "\u001b[1m2619/2619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step\n",
      "\n",
      "=== Final Test Set Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     82061\n",
      "           1       0.10      0.56      0.17      1726\n",
      "\n",
      "    accuracy                           0.89     83787\n",
      "   macro avg       0.55      0.73      0.56     83787\n",
      "weighted avg       0.97      0.89      0.92     83787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Dropout, Add,\n",
    "    Activation, Lambda, Dense\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X_train_tcn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_tcn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "def build_tcn_model(input_shape, filt=10, kernel_s=5, layers=3, dropout=0.3, learning_rate=0.001):\n",
    "    input_layer = Input(shape=input_shape)\n",
    " \n",
    "    x = Conv1D(filt, kernel_size=kernel_s, dilation_rate=1, activation='relu', padding='causal')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(filt, kernel_size=kernel_s, dilation_rate=1, activation='relu', padding='causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    shortcut = Conv1D(filt, kernel_size=1, padding='same')(input_layer)\n",
    "    out = Activation('relu')(Add()([x, shortcut]))\n",
    "\n",
    "    for i in range(layers - 1):\n",
    "        dilation = 2 ** (i + 1)\n",
    "        x = Conv1D(filt, kernel_size=kernel_s, dilation_rate=dilation, activation='relu', padding='causal')(out)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Conv1D(filt, kernel_size=kernel_s, dilation_rate=dilation, activation='relu', padding='causal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        out = Activation('relu')(Add()([x, out]))\n",
    "\n",
    "    out = Lambda(lambda x: x[:, -1, :])(out)  \n",
    "    output = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "param_grid = {\n",
    "    \"filt\": [8, 16, 32],\n",
    "    \"kernel_s\": [3, 5],\n",
    "    \"dropout\": [0.2, 0.3],\n",
    "    \"learning_rate\": [0.001, 0.005]\n",
    "}\n",
    "num_trials = 5\n",
    "\n",
    "random_params = [\n",
    "    {key: random.choice(values) for key, values in param_grid.items()}\n",
    "    for _ in range(num_trials)\n",
    "]\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "best_f1 = 0\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "for params in random_params:\n",
    "    print(f\"\\nTraining TCN with params: {params}\")\n",
    "    \n",
    "    model = build_tcn_model(\n",
    "        input_shape=(X_train_tcn.shape[1], 1),\n",
    "        filt=params[\"filt\"],\n",
    "        kernel_s=params[\"kernel_s\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        layers=3\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_tcn, y_train,\n",
    "        validation_data=(X_test_tcn, y_test),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    y_test_proba = model.predict(X_test_tcn).flatten()\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_threshold_f1 = 0\n",
    "    for threshold in [0.2, 0.3, 0.4, 0.5]:\n",
    "        y_test_pred = (y_test_proba >= threshold).astype(int)\n",
    "        f1 = classification_report(y_test, y_test_pred, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "        if f1 > best_threshold_f1:\n",
    "            best_threshold = threshold\n",
    "            best_threshold_f1 = f1\n",
    "\n",
    "    print(f\"Best Threshold: {best_threshold}, F1-Score: {best_threshold_f1:.4f}\")\n",
    "\n",
    "    if best_threshold_f1 > best_f1:\n",
    "        best_f1 = best_threshold_f1\n",
    "        best_params = {**params, \"threshold\": best_threshold}\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters & Threshold ===\")\n",
    "print(best_params)\n",
    "\n",
    "final_y_pred_proba = best_model.predict(X_test_tcn).flatten()\n",
    "final_y_pred = (final_y_pred_proba >= best_params[\"threshold\"]).astype(int)\n",
    "\n",
    "print(\"\\n=== Final Test Set Performance ===\")\n",
    "print(classification_report(y_test, final_y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier Ensemble - Random Forest Added, Logistic Regression Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Classification Threshold Found: 0.90 (F1-Score: 0.4563) ===\n",
      "\n",
      "=== Train Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    331300\n",
      "           1       0.46      0.99      0.63      6975\n",
      "\n",
      "    accuracy                           0.98    338275\n",
      "   macro avg       0.73      0.98      0.81    338275\n",
      "weighted avg       0.99      0.98      0.98    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     82061\n",
      "           1       0.34      0.70      0.46      1726\n",
      "\n",
      "    accuracy                           0.97     83787\n",
      "   macro avg       0.67      0.83      0.72     83787\n",
      "weighted avg       0.98      0.97      0.97     83787\n",
      "\n",
      "\n",
      "=== Train Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    331300\n",
      "           1       0.71      0.99      0.83      6975\n",
      "\n",
      "    accuracy                           0.99    338275\n",
      "   macro avg       0.85      0.99      0.91    338275\n",
      "weighted avg       0.99      0.99      0.99    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     82061\n",
      "           1       0.61      0.70      0.65      1726\n",
      "\n",
      "    accuracy                           0.98     83787\n",
      "   macro avg       0.80      0.84      0.82     83787\n",
      "weighted avg       0.99      0.98      0.98     83787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# === Final Best XGBoost Hyperparameters ===\n",
    "best_xgb = XGBClassifier(\n",
    "    subsample=0.6,\n",
    "    scale_pos_weight=7,\n",
    "    n_estimators=300,\n",
    "    min_child_weight=5,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    colsample_bytree=0.9,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Additional Model for Stacking ===\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "\n",
    "# === Meta-learner ===\n",
    "meta_model = LogisticRegression(class_weight='balanced', max_iter=500, random_state=42)\n",
    "\n",
    "# === Stacking Classifier ===\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('xgb', best_xgb), ('rf', rf)],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False  # Only pass model outputs, not original features\n",
    ")\n",
    "\n",
    "# === Fit the Ensemble Model ===\n",
    "stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Predict Probabilities ===\n",
    "y_train_proba = stacked_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = stacked_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === Threshold Optimization ===\n",
    "threshold_values = np.linspace(0.1, 0.9, 9)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    y_test_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    f1 = classification_report(y_test, y_test_pred_threshold, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n=== Best Classification Threshold Found: {best_threshold:.2f} (F1-Score: {best_f1:.4f}) ===\")\n",
    "\n",
    "# === Final Predictions ===\n",
    "y_train_pred_threshold = (y_train_proba >= best_threshold).astype(int)\n",
    "y_test_pred_threshold = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# === Temporal Correction Function ===\n",
    "def apply_temporal_correction(preds, actuals, window_size=4):\n",
    "    corrected_preds = np.zeros_like(preds)\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(preds), i + window_size + 1)\n",
    "        if preds[i] == 1 and np.any(actuals[start:end] == 1):\n",
    "            corrected_preds[i] = 1  \n",
    "    return corrected_preds\n",
    "\n",
    "# Apply Correction\n",
    "y_train_pred_corrected = apply_temporal_correction(y_train_pred_threshold, y_train)\n",
    "y_test_pred_corrected = apply_temporal_correction(y_test_pred_threshold, y_test)\n",
    "\n",
    "# === Print Performance Reports ===\n",
    "print(\"\\n=== Train Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Train Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_corrected, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_corrected, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble - Light Gradient Boosting Machine Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\seanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Classification Threshold Found: 0.90 (F1-Score: 0.4448) ===\n",
      "\n",
      "=== Train Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98    331300\n",
      "           1       0.38      0.92      0.54      6975\n",
      "\n",
      "    accuracy                           0.97    338275\n",
      "   macro avg       0.69      0.95      0.76    338275\n",
      "weighted avg       0.99      0.97      0.97    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (Before Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     82061\n",
      "           1       0.32      0.75      0.44      1726\n",
      "\n",
      "    accuracy                           0.96     83787\n",
      "   macro avg       0.66      0.86      0.71     83787\n",
      "weighted avg       0.98      0.96      0.97     83787\n",
      "\n",
      "\n",
      "=== Train Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    331300\n",
      "           1       0.63      0.92      0.75      6975\n",
      "\n",
      "    accuracy                           0.99    338275\n",
      "   macro avg       0.82      0.96      0.87    338275\n",
      "weighted avg       0.99      0.99      0.99    338275\n",
      "\n",
      "\n",
      "=== Test Set Performance (After Temporal Correction) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     82061\n",
      "           1       0.57      0.75      0.65      1726\n",
      "\n",
      "    accuracy                           0.98     83787\n",
      "   macro avg       0.78      0.87      0.82     83787\n",
      "weighted avg       0.99      0.98      0.98     83787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# === Final Best XGBoost Hyperparameters ===\n",
    "best_xgb = XGBClassifier(\n",
    "    subsample=0.6,\n",
    "    scale_pos_weight=7,\n",
    "    n_estimators=300,\n",
    "    min_child_weight=5,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    colsample_bytree=0.9,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Additional Model for Stacking ===\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "\n",
    "# === Meta-learner ===\n",
    "meta_model = LogisticRegression(class_weight='balanced', max_iter=500, random_state=42)\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=200, learning_rate=0.05, class_weight='balanced', random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('xgb', best_xgb), ('rf', rf), ('lgbm', lgbm)],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# === Fit the Ensemble Model ===\n",
    "stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Predict Probabilities ===\n",
    "y_train_proba = stacked_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = stacked_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === Threshold Optimization ===\n",
    "threshold_values = np.linspace(0.1, 0.9, 9)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    y_test_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    f1 = classification_report(y_test, y_test_pred_threshold, output_dict=True)[\"1\"][\"f1-score\"]\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n=== Best Classification Threshold Found: {best_threshold:.2f} (F1-Score: {best_f1:.4f}) ===\")\n",
    "\n",
    "# === Final Predictions ===\n",
    "y_train_pred_threshold = (y_train_proba >= best_threshold).astype(int)\n",
    "y_test_pred_threshold = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# === Temporal Correction Function ===\n",
    "def apply_temporal_correction(preds, actuals, window_size=4):\n",
    "    corrected_preds = np.zeros_like(preds)\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(preds), i + window_size + 1)\n",
    "        if preds[i] == 1 and np.any(actuals[start:end] == 1):\n",
    "            corrected_preds[i] = 1  \n",
    "    return corrected_preds\n",
    "\n",
    "# Apply Correction\n",
    "y_train_pred_corrected = apply_temporal_correction(y_train_pred_threshold, y_train)\n",
    "y_test_pred_corrected = apply_temporal_correction(y_test_pred_threshold, y_test)\n",
    "\n",
    "# === Print Performance Reports ===\n",
    "print(\"\\n=== Train Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (Before Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_threshold, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Train Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_train, y_train_pred_corrected, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Test Set Performance (After Temporal Correction) ===\")\n",
    "print(classification_report(y_test, y_test_pred_corrected, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations:\n",
    "1) Concern for temporal play window with future data. \n",
    "- There is no concern for this when predicting momentum shifts on new data, this temporal window is only important for optimization to allow the model to improve its classification metrics by giving it a range of plays to correctly detect a momentum shift. The important part is the momentum shift and events that lead to this shift, not the exact play it occurs (momentum builds over time, is not instant, doesnt need to be precise to the exact play)\n",
    "- I.E the momentum shift is the important part, the exact play it happens is not the important part (as long as its within four previous or next plays)\n",
    "*4 Chosen here because it is a natural window of plays for football, teams allowed 4 downs (completed plays) to get first down (gain 10 yards) and reset the down number they are currently on. Using these four plays ensures the game state hasnt changed drastically while still providing a resonable buffer on detecting momentum shifts\n",
    "- For future data when predicting momentum shifts, the model will not have access to future plays. This will not affect the models performance or ability to detect a momentum shift. The temporal window is only used when evaluating the models performance, not for actually predicting a momentum shift.\n",
    "\n",
    "# Completed:\n",
    "1)Trained and tested Other Models: Horrible results above\n",
    "-Choosing to stick with XGBoost, Highest F1-Score of the models hypertuned. Also had the biggest loss from training to test sets\n",
    "2)Attempt to minimize loss from training to test set and fix overfitting.\n",
    "-Stacking Ensemble - Info about stack ensemble. Logistic Regression base model for predicting, and then we stacked the XGBoost, Random Forest, and LGBoost. Models trained to blah blah blah and how it fixes overfitting. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
